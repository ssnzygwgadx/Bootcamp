{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Again (this time it's ML flavour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll redo our linear regression and this time we'll make it more \"machine-learning-y\". Let's first reload all the data (etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn or !pip install --user scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd  \n",
    "import seaborn as sns \n",
    "%matplotlib inline  \n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "boston_dataset = load_boston()\n",
    "\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston_Y = boston_dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, arguably, all we need to do is via splitting our data into training and test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n",
      "(102, 13)\n",
      "(404,)\n",
      "(102,)\n"
     ]
    }
   ],
   "source": [
    "# split data into training and test\n",
    "from sklearn.model_selection  import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(boston, boston_Y, test_size = 0.2)\n",
    "\n",
    "# print the shapes to check everything is OK\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"shapes\" shown here are number of rows by number of columns ... i.e. the shape of \"X_train\" is 404 rows by 13 columns. We'd expect the 13 columns (13 X's), and we can quickly do some math on the 506 row numbers ... $ 506 \\hspace{0.2cm} / \\hspace{0.2cm} 5 = 101.2 $ ... to see that 404 is rougly 80 % of the data (our \"test_size\" was 20% so our training data should be 80%) and that this all looks correct. The Y \"shapes\" show only rows but this is because we have only 1x value (1x column).\n",
    "\n",
    "Using these subsets we can train the Linear Regression model on \"X_train\" and \"Y_train\", and then use this model to predict \"X_test\". If we can compare the predictions on \"X_test\" with the real values of \"Y_test\" this gives us a measure of how well our computer has learned to predict house prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance\n",
      "--------------------------------------\n",
      "RMSE is 4.517076436928825\n",
      "R2 score is 0.6985667852793457\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_model = LinearRegression()\n",
    "\n",
    "# fit the model to the training data\n",
    "lin_model_fit = lin_model.fit(X_train, Y_train)\n",
    "\n",
    "# predict the data\n",
    "boston_predict = lin_model_fit.predict(X_test)\n",
    "\n",
    "# calculate RMSE (root mean square error) and R^2 (predictive power)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test, boston_predict)))\n",
    "r2 = r2_score(Y_test, boston_predict)\n",
    "\n",
    "# print the performance metrics\n",
    "print(\"Model performance\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous Notebook our RMSE is lower and $ R^2 $ higher (i.e. better in both cases)! This is a little suprising but probably just chance on how the data was split (random error means the metrics are superior). However, we can conclude that this hasn't made our model worse.\n",
    "\n",
    "What it has done, however, is meant we have a model that is tested against its ability to make predictions on unseen data ... i.e. that it has learned about the process of predicting house prices and can do this well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularisation (LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ L1 $ Regression is performed in much the same way as Linear Regression. We have one other value (a hyperparameter - more on these later) of $ a $ which determines how much influence the $ L1 $ penalty has on how the line is fit in the model. We will dodge the issue of what this should be for now and just randomly set it as 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance\n",
      "--------------------------------------\n",
      "RMSE is 4.665022866200911\n",
      "R2 score is 0.6784979254792699\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "l1_model = Lasso(alpha=0.5)\n",
    "\n",
    "# fit the model to the training data\n",
    "l1_model_fit = l1_model.fit(X_train, Y_train)\n",
    "\n",
    "# predict the data\n",
    "boston_predict = l1_model_fit.predict(X_test)\n",
    "\n",
    "# calculate RMSE (root mean square error) and R^2 (predictive power)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test, boston_predict)))\n",
    "r2 = r2_score(Y_test, boston_predict)\n",
    "\n",
    "# print the performance metrics\n",
    "print(\"Model performance\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance is slightly lower than for our normal linear model. __However...__ this is somewhat to be expected and actually performance is only slightly worse. This may be a small price to pay for ensuring that our model is a little more robust to changing data.\n",
    "\n",
    "Where we should see a much more fundamental difference is in the beta-weights/coefficients of the two models (the $ b $ values). Let's compare the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta weights/co-efficients - UNREGULARISED\n",
      "-----------------------------------------\n",
      "CRIM: -0.121\n",
      "ZN: 0.0569\n",
      "INDUS: 0.0198\n",
      "CHAS: 2.3179\n",
      "NOX: -18.1873\n",
      "RM: 3.807\n",
      "AGE: 0.0\n",
      "DIS: -1.642\n",
      "RAD: 0.3\n",
      "TAX: -0.0112\n",
      "PTRATIO: -0.9878\n",
      "B: 0.0098\n",
      "LSTAT: -0.5462\n",
      "\n",
      "\n",
      "Beta weights/co-efficients - LASSO\n",
      "-----------------------------------------\n",
      "CRIM: -0.098\n",
      "ZN: 0.0603\n",
      "INDUS: -0.0042\n",
      "CHAS: 0.0\n",
      "NOX: -0.0\n",
      "RM: 2.5633\n",
      "AGE: 0.0\n",
      "DIS: -1.0953\n",
      "RAD: 0.2659\n",
      "TAX: -0.0144\n",
      "PTRATIO: -0.8029\n",
      "B: 0.0095\n",
      "LSTAT: -0.667\n"
     ]
    }
   ],
   "source": [
    "# print the beta values of the model (co-efficients)\n",
    "betas = lin_model_fit.coef_\n",
    "counter = 0\n",
    "for col in boston.columns:\n",
    "    if counter == 0:\n",
    "        print(\"Beta weights/co-efficients - UNREGULARISED\")\n",
    "        print(\"-----------------------------------------\")\n",
    "    print(col + \": \" + str(round(betas[counter], 4)))\n",
    "    counter +=1\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "# print the beta values of the model (co-efficients)\n",
    "betas_l1 = l1_model_fit.coef_\n",
    "counter = 0\n",
    "for col in boston.columns:\n",
    "    if counter == 0:\n",
    "        print(\"Beta weights/co-efficients - LASSO\")\n",
    "        print(\"-----------------------------------------\")\n",
    "    print(col + \": \" + str(round(betas_l1[counter], 4)))\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see some of the beta weights are now 0 (\"INDUS\", \"CHAS\" and \"NOX\" - of which \"CHAS\" and \"NOX\" were fairly large weights in the original model). We have effectively removed these features (X's) from the model - they are now (for instance) $ 0 * INDUS $ which will obviously be zero and therefore won't change the calculation of $ Y $. We have performed _feature selection_ - removing X's from the model.\n",
    "\n",
    "We have also minimised the impact (weight) of all the X's. For instance, \"RM\" had a beta of 3.77 in the original model and only 0.74 in the LASSO model.\n",
    "\n",
    "Overall we have made the model less reliant on certain features and removed others entirely (reducing model complexity). Both these elements mean our model is more robust/protected against over-fitting ... and the cost is only 0.05 in terms of $ R^2 $ score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularisation (Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularisation is performed in a very similar way in _scikit-learn_ ... and again we will set the hyperparameter $ a $ to the arbitrary value of 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance\n",
      "--------------------------------------\n",
      "RMSE is 4.5205862598769695\n",
      "R2 score is 0.698098168792698\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "l2_model = Ridge(alpha=0.5)\n",
    "\n",
    "# fit the model to the training data\n",
    "l2_model_fit = l2_model.fit(X_train, Y_train)\n",
    "\n",
    "# predict the data\n",
    "boston_predict = l2_model_fit.predict(X_test)\n",
    "\n",
    "# calculate RMSE (root mean square error) and R^2 (predictive power)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test, boston_predict)))\n",
    "r2 = r2_score(Y_test, boston_predict)\n",
    "\n",
    "# print the performance metrics\n",
    "print(\"Model performance\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have model performance that slightly improves upon the original model ... while also adding regularisation protection. Let's compare the three beta weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta weights/co-efficients - UNREGULARISED\n",
      "-----------------------------------------\n",
      "CRIM: -0.121\n",
      "ZN: 0.0569\n",
      "INDUS: 0.0198\n",
      "CHAS: 2.3179\n",
      "NOX: -18.1873\n",
      "RM: 3.807\n",
      "AGE: 0.0\n",
      "DIS: -1.642\n",
      "RAD: 0.3\n",
      "TAX: -0.0112\n",
      "PTRATIO: -0.9878\n",
      "B: 0.0098\n",
      "LSTAT: -0.5462\n",
      "\n",
      "\n",
      "Beta weights/co-efficients - LASSO\n",
      "-----------------------------------------\n",
      "CRIM: -0.098\n",
      "ZN: 0.0603\n",
      "INDUS: -0.0042\n",
      "CHAS: 0.0\n",
      "NOX: -0.0\n",
      "RM: 2.5633\n",
      "AGE: 0.0\n",
      "DIS: -1.0953\n",
      "RAD: 0.2659\n",
      "TAX: -0.0144\n",
      "PTRATIO: -0.8029\n",
      "B: 0.0095\n",
      "LSTAT: -0.667\n",
      "\n",
      "\n",
      "Beta weights/co-efficients - RIDGE\n",
      "-----------------------------------------\n",
      "CRIM: -0.1184\n",
      "ZN: 0.0576\n",
      "INDUS: -0.0031\n",
      "CHAS: 2.2508\n",
      "NOX: -12.7986\n",
      "RM: 3.8257\n",
      "AGE: -0.0049\n",
      "DIS: -1.5645\n",
      "RAD: 0.2875\n",
      "TAX: -0.0116\n",
      "PTRATIO: -0.9325\n",
      "B: 0.01\n",
      "LSTAT: -0.5526\n"
     ]
    }
   ],
   "source": [
    "# print the beta values of the model (co-efficients)\n",
    "betas = lin_model_fit.coef_\n",
    "counter = 0\n",
    "for col in boston.columns:\n",
    "    if counter == 0:\n",
    "        print(\"Beta weights/co-efficients - UNREGULARISED\")\n",
    "        print(\"-----------------------------------------\")\n",
    "    print(col + \": \" + str(round(betas[counter], 4)))\n",
    "    counter +=1\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "# print the beta values of the model (co-efficients)\n",
    "betas_l1 = l1_model_fit.coef_\n",
    "counter = 0\n",
    "for col in boston.columns:\n",
    "    if counter == 0:\n",
    "        print(\"Beta weights/co-efficients - LASSO\")\n",
    "        print(\"-----------------------------------------\")\n",
    "    print(col + \": \" + str(round(betas_l1[counter], 4)))\n",
    "    counter +=1\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "# print the beta values of the model (co-efficients)\n",
    "betas_l2 = l2_model_fit.coef_\n",
    "counter = 0\n",
    "for col in boston.columns:\n",
    "    if counter == 0:\n",
    "        print(\"Beta weights/co-efficients - RIDGE\")\n",
    "        print(\"-----------------------------------------\")\n",
    "    print(col + \": \" + str(round(betas_l2[counter], 4)))\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with L1/LASSO regression all our beta values are lower than in the original model - and therefore each feature is less influential. However, none have been reduced to zero. L2 cannot perform _feature selection_ (deleting X's) unlike with L1.\n",
    "\n",
    "So better performance and regularisation added - seems like a good deal right? However, before we call it a day we have one other regularisation method we may try. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticNet uses both L1 and L2 regularisation. We now have an additonal value to set (hyperparameter - still going to come back to these) the _l1\\_ratio._ As the name suggests the _l1\\_ratio_ determines the proportion that is L1 versus L2 so \"1\" would be just L1 and \"0\" would just be L2. We'll arbitrarily go with 0.5 again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance\n",
      "--------------------------------------\n",
      "RMSE is 4.733687229395052\n",
      "R2 score is 0.6689639094766657\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "enet_model = ElasticNet(alpha=0.5, l1_ratio=0.5)\n",
    "\n",
    "# fit the model to the training data\n",
    "enet_model_fit = enet_model.fit(X_train, Y_train)\n",
    "\n",
    "# predict the data\n",
    "boston_predict = enet_model_fit.predict(X_test)\n",
    "\n",
    "# calculate RMSE (root mean square error) and R^2 (predictive power)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "rmse = (np.sqrt(mean_squared_error(Y_test, boston_predict)))\n",
    "r2 = r2_score(Y_test, boston_predict)\n",
    "\n",
    "# print the performance metrics\n",
    "print(\"Model performance\")\n",
    "print(\"--------------------------------------\")\n",
    "print('RMSE is {}'.format(rmse))\n",
    "print('R2 score is {}'.format(r2))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly the result is somewhere between the results of L1 and L2. Let's also check the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta weights/co-efficients - UNREGULARISED\n",
      "-----------------------------------------\n",
      "CRIM: -0.121\n",
      "ZN: 0.0569\n",
      "INDUS: 0.0198\n",
      "CHAS: 2.3179\n",
      "NOX: -18.1873\n",
      "RM: 3.807\n",
      "AGE: 0.0\n",
      "DIS: -1.642\n",
      "RAD: 0.3\n",
      "TAX: -0.0112\n",
      "PTRATIO: -0.9878\n",
      "B: 0.0098\n",
      "LSTAT: -0.5462\n",
      "\n",
      "\n",
      "Beta weights/co-efficients - LASSO\n",
      "-----------------------------------------\n",
      "CRIM: -0.098\n",
      "ZN: 0.0603\n",
      "INDUS: -0.0042\n",
      "CHAS: 0.0\n",
      "NOX: -0.0\n",
      "RM: 2.5633\n",
      "AGE: 0.0\n",
      "DIS: -1.0953\n",
      "RAD: 0.2659\n",
      "TAX: -0.0144\n",
      "PTRATIO: -0.8029\n",
      "B: 0.0095\n",
      "LSTAT: -0.667\n",
      "\n",
      "\n",
      "Beta weights/co-efficients - RIDGE\n",
      "-----------------------------------------\n",
      "CRIM: -0.1184\n",
      "ZN: 0.0576\n",
      "INDUS: -0.0031\n",
      "CHAS: 2.2508\n",
      "NOX: -12.7986\n",
      "RM: 3.8257\n",
      "AGE: -0.0049\n",
      "DIS: -1.5645\n",
      "RAD: 0.2875\n",
      "TAX: -0.0116\n",
      "PTRATIO: -0.9325\n",
      "B: 0.01\n",
      "LSTAT: -0.5526\n",
      "\n",
      "\n",
      "Beta weights/co-efficients - ELASTICNET\n",
      "-----------------------------------------\n",
      "CRIM: -0.1063\n",
      "ZN: 0.0648\n",
      "INDUS: -0.0274\n",
      "CHAS: 0.0\n",
      "NOX: -0.0\n",
      "RM: 1.7902\n",
      "AGE: 0.0065\n",
      "DIS: -1.1318\n",
      "RAD: 0.299\n",
      "TAX: -0.0153\n",
      "PTRATIO: -0.8545\n",
      "B: 0.0089\n",
      "LSTAT: -0.7182\n"
     ]
    }
   ],
   "source": [
    "# print the beta values of the model (co-efficients)\n",
    "betas = lin_model_fit.coef_\n",
    "counter = 0\n",
    "for col in boston.columns:\n",
    "    if counter == 0:\n",
    "        print(\"Beta weights/co-efficients - UNREGULARISED\")\n",
    "        print(\"-----------------------------------------\")\n",
    "    print(col + \": \" + str(round(betas[counter], 4)))\n",
    "    counter +=1\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "# print the beta values of the model (co-efficients)\n",
    "betas_l1 = l1_model_fit.coef_\n",
    "counter = 0\n",
    "for col in boston.columns:\n",
    "    if counter == 0:\n",
    "        print(\"Beta weights/co-efficients - LASSO\")\n",
    "        print(\"-----------------------------------------\")\n",
    "    print(col + \": \" + str(round(betas_l1[counter], 4)))\n",
    "    counter +=1\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "# print the beta values of the model (co-efficients)\n",
    "betas_l2 = l2_model_fit.coef_\n",
    "counter = 0\n",
    "for col in boston.columns:\n",
    "    if counter == 0:\n",
    "        print(\"Beta weights/co-efficients - RIDGE\")\n",
    "        print(\"-----------------------------------------\")\n",
    "    print(col + \": \" + str(round(betas_l2[counter], 4)))\n",
    "    counter +=1\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "# print the beta values of the model (co-efficients)\n",
    "betas_enet = enet_model_fit.coef_\n",
    "counter = 0\n",
    "for col in boston.columns:\n",
    "    if counter == 0:\n",
    "        print(\"Beta weights/co-efficients - ELASTICNET\")\n",
    "        print(\"-----------------------------------------\")\n",
    "    print(col + \": \" + str(round(betas_enet[counter], 4)))\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, as we may expect, the results are somewhere between the two. \"CHAS\" and \"NOX\" have betas of zero (have been removed) but not \"INDUS\". All other beta weights are reduced to somewhere between the L1 and L2 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS! Which is best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hahahahaha ... you know what I'm going to say:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### IT DEPENDS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not worried about overfitting use the original model (but if you don't care about overfitting are you doing ML?). If you have a lot of features and want some removed you may use L1 or Elastic Net. If you care most about performance use the L2 model. In this case I'd probably go with L2 as there aren't a lot of features and the performance is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
